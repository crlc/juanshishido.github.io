<!DOCTYPE HTML>
<html>
    <head>
        <title>General Linear Models</title>

        <script src="js/jquery.min.js"></script>
        <script src="js/jquery.scrolly.min.js"></script>
        <script src="js/jquery.scrollgress.min.js"></script>
        <script src="js/skel.min.js"></script>
        <script src="js/skel-layers.min.js"></script>
        <script src="js/init.js"></script>
        <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

        <noscript>
            <link rel="stylesheet" href="css/skel.css" />
            <link rel="stylesheet" href="css/style.css" />
            <link rel="stylesheet" href="css/style-wide.css" />
            <link rel="stylesheet" href="css/style-noscript.css" />
        </noscript>

        <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
        <script src="js/smoothscroll.js"></script>

        <link rel="stylesheet" type="text/css" media="screen"
        href="http://cdnjs.cloudflare.com/ajax/libs/fancybox/1.3.4/jquery.fancybox-1.3.4.css" />
        <style type="text/css">
            a.fancybox img {
                border: none;
            }
        </style>

        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-68428630-1', 'auto');
            ga('send', 'pageview');

        </script>

    </head>


    <body class="index">

       <!-- Header -->
        <header id="header" style="position: fixed; top: 0px; left: 0px; right: 0px; z-index: 1030;">
            <h1 id="logo"><a href="index.html">Home</a></h1>
            <nav id="nav">
                <ul>
                    <li><a href="#top" class="smoothScroll">^</a></li>
                    <li><a href="#contact" class="smoothScroll">Contact</a></li>
                </ul>
            </nav>
        </header>

        <!-- Main -->
        <article id="main">

            <!-- Emoji -->
            <div id="top">
                <section class="wrapper style3 container">

                    <header>
                        <h2><center>General Linear Models</center></h2>
                    </header>

                    <!-- Text -->
                    <h3>Simple Linear Regression</h3>
                    <p>
                        A line can be described by the equation \(y = mx + b\),
                        where \(b\) is the \(y\)-intercept and \(m\) is the
                        slope. Simple linear regression uses this idea to
                        describe the <em>relationship</em> between two
                        variables&mdash;one dependent, the \(y\), and one
                        independent, the \(x\). Dependent variables are also
                        sometimes referred to as "response variables,"
                        "regressands," or "targets." The corresponding terms for
                        independent variables are "predictor variables,"
                        "regressors," or "features."
                    </p>
                    <p>
                        The objective is to specify a model that explains \(y\)
                        in terms of \(x\) in the <em>population</em> of
                        interest. A simple model can be specified as follows.
                    </p>
                    <p>
                        \[y = \beta_0 + \beta_1 x + \epsilon\]
                    </p>
                    <p>
                        In this specification&mdash;called the scalar
                        form&mdash;\(\beta_0\) represents the \(y\)-intercept
                        and \(\beta_1\) represents the slope. The \(\epsilon\),
                        which is called the error term, represents
                        <em>other</em> factors that affect \(y\).
                    </p>
                    <p>
                        In regression, to estimate the \(\beta\)s, we need a
                        sample from the population. We use this data to find the
                        line that "best" fits it. Because each observation in
                        our sample has its own values, we use subscripts to
                        denote them.
                    </p>
                    <p>
                        \[y_i = \beta_0 + \beta_1 x_i + \epsilon_i\]
                    </p>
                    <p>
                        Notice that the \(y\), \(x\), and \(\epsilon\) terms
                        have subscripts, but the \(\beta\)s do not. This implies
                        that there is a <em>single</em> \(y\)-intercept and
                        slope&mdash;the one from the population equation that
                        represents the "true" line.
                    </p>
                    <h3>Matrix Form</h3>
                    <p>
                        The scalar form allows us to be explicit and verbose
                        about the regressors included in the model. However, it
                        is often more efficient to write the equation in matrix
                        form. To motivate the reasons why, consider the case
                        where, for our equation above, \(i \in 1,\ldots,n\).
                    </p>
                    <p>
                        \[y_1 = \beta_0 + \beta_1 x_1 + \epsilon_1\]
                        \[y_2 = \beta_0 + \beta_1 x_2 + \epsilon_2\]
                        \[ \vdots \]
                        \[y_n = \beta_0 + \beta_1 x_n + \epsilon_n\]
                    </p>
                    <p>
                        It becomes inefficient to out \(n\) equations. Instead,
                        we can think of \(y_i\), \(x_i\), and \(\epsilon_i\) as
                        being vectors with \(n\) values. For example, \(y_i\)
                        can be written as \(\vec{y}\). Our equation becomes:
                    </p>
                    <p>
                        \[\vec{y} = \beta_0 + \beta_1 \vec{x} + \vec{\epsilon}\]
                    </p>
                    <p>
                        Using this notation, the \(\beta\) values are scalars.
                        However, with what we've described thus far, \(\beta_0\)
                        doesn't have a vector of values to be multiplied with.
                        So, we add a vector of ones of length \(n\).
                    </p>
                    <p>
                        Our <em>system</em> of \(n\) equations can be
                        represented as the following matrix addition equation.
                    </p>
                    <p>
                        \begin{split}
                            \left[\begin{matrix}y_1\\ \vdots\\ y_n\end{matrix}\right] =
                            \beta_0 \left[\begin{matrix}1\\ \vdots\\ 1\end{matrix}\right] +
                            \beta_1 \left[\begin{matrix}x_1\\ \vdots\\ x_n\end{matrix}\right] +
                            \left[\begin{matrix}\epsilon_1\\ \vdots\\ \epsilon_n\end{matrix}\right]
                        \end{split}
                    </p>
                    <p>
                        We can restate this in terms of matrix multiplication.
                    </p>
                    <p>
                        \begin{split}
                            \left[\begin{matrix}y_1\\ \vdots\\ y_n\end{matrix}\right] =
                            \left[\begin{matrix}1 & x_1\\ \vdots & \vdots\\ 1 & x_n\end{matrix}\right]
                            \left[\begin{matrix}\beta_0\\ \beta_1\end{matrix}\right] +
                            \left[\begin{matrix}\epsilon_1\\ \vdots\\ \epsilon_n\end{matrix}\right]
                        \end{split}
                    </p>
                    <p>
                        Here, we've simply combined the vector of ones and
                        \(x_i\) into a matrix, which we refer to as \(X\),
                        called the <em>design matrix</em>, and
                        combined the \(\beta\)s into a column vector, which
                        we refer to as \(\vec{\beta}\), called the <em>parameter
                        vector</em>. In this example, \(X\) is an
                        \(n \times p\) matrix and \(\vec{\beta}\) can be thought
                        of as a \(p \times 1\) matrix. Because the number of
                        columns in \(X\) equals the number of rows in
                        \(\vec{\beta}\), they can be multiplied.
                    </p>
                    <p>
                        This can be rewritten as follows.
                    </p>
                    <p>
                        \[\vec{y} = X\vec{\beta} + \vec{\epsilon} \]
                    </p>
                    <h3>Multiple Regression</h3>
                    <p>
                        Often, outcomes for our variable of interest, \(y_i\),
                        depend on two or more predictors. When specifying it
                        this way, it is referred to as a <em>multiple</em>
                        linear regression model, which, for the case of two
                        variables, can be written as (in scalar form):
                    </p>
                    <p>
                        \[y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i\]
                    </p>
                    <p>
                        As you might imagine, the scalar form gets unwiedly as
                        the number of independent variables increases. This
                        model, as well as those with an arbitrary number of
                        parameters, can still be expressed in matrix form.
                    </p>
                    <p>
                        \[\vec{y} = X\vec{\beta} + \vec{\epsilon}\]
                    </p>
                        For each parameter, \(p\), add a column to \(X\) and an
                        element to \(\vec{\beta}\).
                    </p>
                    <h3>Solving</h3>
                    <p>
                        There are several ways to solve for the "best" line. To
                        illustrate this, we'll go back to the simple linear
                        regression model. Let's suppose we'd like to create a
                        regression line for the data shown in the following
                        plot, created using the code below. (Note: the complete
                        set of commands used to make this plot can be found
                        <a href="https://gist.github.com/juanshishido/e14209aaa26b96b756ee#file-slr_data-py"
                        target="_blank">here</a>.)
                    </p>
<pre>
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(1868)
x = np.random.normal(0, 1, 20)
y = np.array([np.random.normal(v, 0.5) for v in x])

plt.scatter(x, y, s=35, alpha=0.85, color='#328E82')
</pre>
                    <p>
                        <center>
                            <img src="images/slr-data.png" class="fancybox"
                            style="border:1px solid lightgray; width:90%;">
                        </center>
                    </p>
                    <p>
                        The "best" line is usually thought of as the one that
                        <em>minimizes</em> the distance between it and the data
                        points. Typically, the distance that is minimized is the
                        sum of the squared residuals. Residuals are the
                        difference between each \(y_i\) and its estimated (or
                        fitted) value, \(\hat{y_i}\). In vector notation,
                        these can be written as \(\vec{y}\) and
                        \(\hat{\vec{y}}\), respectively. The idea is to find
                        values of \(\beta_0\) and \(\beta_1\) that minimize:
                        \[ \sum_{i=1}^n (y_i - \hat{y_i})^2\]
                    </p>
                    <p>
                        A side note before we continue. The terms "errors" and
                        "residuals" are often used as synonyms, unfortunately.
                        In reality, they are quite distinct. Errors are a
                        population-level concept and are unobservable.
                        Residuals, on the other hand, are sample-specific and
                        are computed from the data when we estimate the
                        \(\beta\)s.
                    </p>
                    <p>
                        Back to estimating our \(\beta\)s. In the case of simple
                        linear regression, the estimated slope is
                        \[ \hat{\beta_1} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{cov(x_i, y_i)}{var(x_i)}\]
                        and the estimated \(y\)-intercept is
                        \[ \hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x} \]
                    </p>
                    <p>
                        Using NumPy, we can calculate these values as follows.
                    </p>
<pre>
b1 = np.cov(x, y, bias=1)[0,1] / np.var(x)
b0 = y.mean() - b1*x.mean()

</pre>
                    <p>
                        This results in the following regression line, in which
                        the sum of the squared distances between the data
                        points and the line have been minimized.
                    </p>
                    <p>
                        <center>
                            <img src="images/slr-line.png" class="fancybox"
                            style="border:1px solid lightgray; width:90%;">
                        </center>
                    </p>
                    <p>
                        To solve for \(\vec{\beta}\) in the case multiple linear
                        regression....
                    </p>
                    <p>
                        \[ \hat{\vec{\beta}} = (X^T X)^{-1} X^T \vec{y}\]
                    </p>

                </section>
            </div>
        </article>

        <!-- Footer -->
        <div id="contact">
            <footer id="footer">

                <h2><strong>Contact</strong></h2>
                <ul class="icons">
                    <li>
                        <a href="http://www.linkedin.com/in/juanshishido/"
                        class="icon circle fa-linkedin" target="_blank">
                        <span class="label">LinkedIn</span></a>
                    </li>
                    <li>
                        <a href="https://twitter.com/juanshishido"
                        class="icon circle fa-twitter" target="_blank">
                        <span class="label">Twitter</span></a>
                    </li>
                    <li>
                        <a href="https://github.com/juanshishido"
                        class="icon circle fa-github" target="_blank">
                        <span class="label">Github</span></a>
                    </li>
                </ul>

                <ul class="copyright">
                    <li>
                        <a href="index.html" class="smoothScroll">Juan Shishido</a>
                    </li>
                    <li>Design:
                        <a href="http://html5up.net" target="_blank">HTML5 UP</a>
                    </li>
                </ul>

            </footer>
        </div>

        <script type="text/javascript"
        src="http://code.jquery.com/jquery-1.11.0.min.js"></script>
        <script type="text/javascript"
        src="http://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
        <script type="text/javascript"
        src="http://cdnjs.cloudflare.com/ajax/libs/fancybox/1.3.4/jquery.fancybox-1.3.4.pack.min.js"></script>
        <script type="text/javascript">
            $(function($){
                var addToAll = false;
                var gallery = false;
                var titlePosition = 'over';
                $(addToAll ? 'img' : 'img.fancybox').each(function(){
                    var $this = $(this);
                    var title = $this.attr('title');
                    var src = $this.attr('data-big') || $this.attr('src');
                    var a = $('<a href="#" class="fancybox"></a>').attr('href', src).attr('title', title);
                    $this.wrap(a);
                });
                if (gallery)
                    $('a.fancybox').attr('rel', 'fancyboxgallery');
                $('a.fancybox').fancybox({
                    titlePosition: titlePosition
                });
            });
            $.noConflict();
        </script>

    </body>
</html>
